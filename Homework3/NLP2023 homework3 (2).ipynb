{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6e0166",
   "metadata": {},
   "source": [
    "# Natural Language Processing 2023-1A Homework 3\n",
    "\n",
    "# Vector Semantics and Word2Vec \n",
    "\n",
    "Deadline: 27 September (23:59)\n",
    "\n",
    "Questions: Post them in the HW3 discussion on Canvas, sent them to nlp-course@utwente.nl or ask us during the practical sessions. \n",
    "\n",
    "How to submit: Please answer the questions directly in this notebook and submit it before the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050ad9bd",
   "metadata": {},
   "source": [
    "## Please Write your group number, your names with student IDs Here: \n",
    "Group 10 \n",
    "\n",
    "Student Name and student Id: Hafsa Azhar s3099849\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5705bb26",
   "metadata": {},
   "source": [
    "Make sure that the following libraries are up-to-date in your computation envrionment. It is highly recommended to work on this assignment in UT's [JupyterLab](https://www.utwente.nl/en/service-portal/research-support/it-facilities-for-research/jupyterlab). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeaaa5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: sklearn in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.0.post9)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: pyfume in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: simpful in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.11.0)\n",
      "Requirement already satisfied: fst-pso in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: miniful in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '###'\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gensim nltk sklearn numpy pandas scipy \n",
    "!pip install  --upgrade gensim nltk sklearn numpy pandas scipy ### Upgrade your libraries if neccesary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24ba63-c517-4ecf-bb9a-adce51f079b8",
   "metadata": {},
   "source": [
    "We'll need these libraries later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e976edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas, numpy, scipy, math\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a13784d",
   "metadata": {},
   "source": [
    "In this assignment, you will explore two types of word vectors: those derived from **co-occurrence count-based methods**, and those derived via the **local context predictive model word2vec**. \n",
    "\n",
    "Note on Terminology: \n",
    "- The word \"word\" and \"term\" are used interchangeably here. They both mean the unique tokens that you would like to represent in terms of vectors. Often they are individual words, n-grams,  phrases, or even identifiers. In the assignment, we focus on individual words. \n",
    "- The terms \"word vectors\" and \"word embeddings\" are often used interchangeably, but they are actually different. According to [Wikipedia](https://en.wikipedia.org/wiki/Word_embedding), conceptually, word embedding \"*involves the mathematical embedding from space with many dimensions per word to a continuous vector space with a much lower dimension*\".\n",
    "\n",
    "# Part I. Co-occurrence count-based vectors\n",
    "\n",
    "Let's start with this corpus consisting of 5 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0db5b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents=['Simultaneous heatwaves at North and South Poles.',\n",
    "        'Both North and South poles see unusual high heat.',\n",
    "        'North pole is heating up. What about South pole?',\n",
    "        'Bizarre heatwaves strike Arctic and Antarctic poles.',\n",
    "        'Climate change at the poles.'\n",
    "       ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e93419f",
   "metadata": {},
   "source": [
    "### Exercise 1.1.1 Construct the vocabulary (0.5 point)\n",
    "Before we construct co-occurrence matrices, we need to identify unique terms in the corpus, i.e. construct the vocabulary. You can remove stop words and apply other text normalisation operations before constructing the vocabulary. \n",
    "\n",
    "Tip: Sort your vocabulary alphabetically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b11e7e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary is 17\n",
      "The words in the vocabulary are ['antarctic', 'arctic', 'bizarre', 'change', 'climate', 'heat', 'heating', 'heatwaves', 'high', 'north', 'pole', 'poles', 'see', 'simultaneous', 'south', 'strike', 'unusual']\n"
     ]
    }
   ],
   "source": [
    "# Creating a CountVectorizer with stop words and sorting it\n",
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english')).fit(sents)\n",
    "vocab = [*vectorizer.vocabulary_]\n",
    "vocab = sorted(vocab, reverse=False)\n",
    "\n",
    "# your code ends here\n",
    "\n",
    "print('The size of the vocabulary is', len(vocab))\n",
    "print('The words in the vocabulary are', vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9245a",
   "metadata": {},
   "source": [
    "### Co-Occurrence\n",
    "\n",
    "A co-occurrence matrix counts how often terms co-occur in certain context. The context can be a complete document, a sentence, or a sliding window. \n",
    "\n",
    "Tip: Check out the [sklearn.feature_extraction.text](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text) submodule that gathers utilities to build feature vectors from text documents. \n",
    "\n",
    "### Exercise 1.1.2 Term-document occurrence matrix and term-term co-occurrence matrix (0.5 point)\n",
    "Let's first consider **each sentence** in the above corpus to be the context where the (co-)occurrences are counted. For example, the words *heatwaves*, *north*, *south* and *poles* occur in the first sentence, therefore, they occur in this document and co-occur with each other. Going through all the sentences, you can construct the term-document occurrence matrix and term-term co-occurrence matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "111b9b50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the term-document matrix is (17, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>antarctic</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arctic</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bizarre</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>change</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>climate</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heat</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heating</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heatwaves</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pole</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poles</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simultaneous</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>south</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strike</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unusual</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              1  2  3  4  5\n",
       "antarctic     0  0  0  1  0\n",
       "arctic        0  0  0  1  0\n",
       "bizarre       0  0  0  1  0\n",
       "change        0  0  0  0  1\n",
       "climate       0  0  0  0  1\n",
       "heat          0  1  0  0  0\n",
       "heating       0  0  1  0  0\n",
       "heatwaves     1  0  0  1  0\n",
       "high          0  1  0  0  0\n",
       "north         1  1  1  0  0\n",
       "pole          0  0  1  0  0\n",
       "poles         1  1  0  1  1\n",
       "see           0  1  0  0  0\n",
       "simultaneous  1  0  0  0  0\n",
       "south         1  1  1  0  0\n",
       "strike        0  0  0  1  0\n",
       "unusual       0  1  0  0  0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the term-document occurrence matrix: tdMatrix_pd\n",
    "\n",
    "count_vectorizer= CountVectorizer(vocabulary=vocab, binary=True)\n",
    "tdMatrix = count_vectorizer.transform(sents)\n",
    "# to match the expected shape \n",
    "tdMatrix = tdMatrix.transpose()\n",
    "# your code ends here\n",
    "\n",
    "print('The shape of the term-document matrix is', tdMatrix.shape)\n",
    "tdMatrix_pd = pandas.DataFrame(tdMatrix.toarray(), index=vocab, columns=range(1, len(sents) + 1))\n",
    "tdMatrix_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980624f5",
   "metadata": {},
   "source": [
    "Tip: You can derive the term-term co-occurrence matrix directly from the term-document occurrence matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "374d9fe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the term-term matrix is (17, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antarctic</th>\n",
       "      <th>arctic</th>\n",
       "      <th>bizarre</th>\n",
       "      <th>change</th>\n",
       "      <th>climate</th>\n",
       "      <th>heat</th>\n",
       "      <th>heating</th>\n",
       "      <th>heatwaves</th>\n",
       "      <th>high</th>\n",
       "      <th>north</th>\n",
       "      <th>pole</th>\n",
       "      <th>poles</th>\n",
       "      <th>see</th>\n",
       "      <th>simultaneous</th>\n",
       "      <th>south</th>\n",
       "      <th>strike</th>\n",
       "      <th>unusual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>antarctic</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arctic</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bizarre</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>change</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>climate</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heat</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heating</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heatwaves</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pole</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poles</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simultaneous</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>south</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strike</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unusual</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              antarctic  arctic  bizarre  change  climate  heat  heating  \\\n",
       "antarctic             0       1        1       0        0     0        0   \n",
       "arctic                1       0        1       0        0     0        0   \n",
       "bizarre               1       1        0       0        0     0        0   \n",
       "change                0       0        0       0        1     0        0   \n",
       "climate               0       0        0       1        0     0        0   \n",
       "heat                  0       0        0       0        0     0        0   \n",
       "heating               0       0        0       0        0     0        0   \n",
       "heatwaves             1       1        1       0        0     0        0   \n",
       "high                  0       0        0       0        0     1        0   \n",
       "north                 0       0        0       0        0     1        1   \n",
       "pole                  0       0        0       0        0     0        1   \n",
       "poles                 1       1        1       1        1     1        0   \n",
       "see                   0       0        0       0        0     1        0   \n",
       "simultaneous          0       0        0       0        0     0        0   \n",
       "south                 0       0        0       0        0     1        1   \n",
       "strike                1       1        1       0        0     0        0   \n",
       "unusual               0       0        0       0        0     1        0   \n",
       "\n",
       "              heatwaves  high  north  pole  poles  see  simultaneous  south  \\\n",
       "antarctic             1     0      0     0      1    0             0      0   \n",
       "arctic                1     0      0     0      1    0             0      0   \n",
       "bizarre               1     0      0     0      1    0             0      0   \n",
       "change                0     0      0     0      1    0             0      0   \n",
       "climate               0     0      0     0      1    0             0      0   \n",
       "heat                  0     1      1     0      1    1             0      1   \n",
       "heating               0     0      1     1      0    0             0      1   \n",
       "heatwaves             0     0      1     0      2    0             1      1   \n",
       "high                  0     0      1     0      1    1             0      1   \n",
       "north                 1     1      0     1      2    1             1      3   \n",
       "pole                  0     0      1     0      0    0             0      1   \n",
       "poles                 2     1      2     0      0    1             1      2   \n",
       "see                   0     1      1     0      1    0             0      1   \n",
       "simultaneous          1     0      1     0      1    0             0      1   \n",
       "south                 1     1      3     1      2    1             1      0   \n",
       "strike                1     0      0     0      1    0             0      0   \n",
       "unusual               0     1      1     0      1    1             0      1   \n",
       "\n",
       "              strike  unusual  \n",
       "antarctic          1        0  \n",
       "arctic             1        0  \n",
       "bizarre            1        0  \n",
       "change             0        0  \n",
       "climate            0        0  \n",
       "heat               0        1  \n",
       "heating            0        0  \n",
       "heatwaves          1        0  \n",
       "high               0        1  \n",
       "north              0        1  \n",
       "pole               0        0  \n",
       "poles              1        1  \n",
       "see                0        1  \n",
       "simultaneous       0        0  \n",
       "south              0        1  \n",
       "strike             0        0  \n",
       "unusual            0        0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code starts here\n",
    "# Convert tdMatrix to a NumPy array\n",
    "tdMatrix = tdMatrix.toarray()\n",
    "\n",
    "# Construct the term-term co-occurrence matrix\n",
    "ttMatrix = tdMatrix.dot(tdMatrix.T)\n",
    "numpy.fill_diagonal(ttMatrix, 0)\n",
    "# your code ends here\n",
    "print('The shape of the term-term matrix is', ttMatrix.shape)\n",
    "ttMatrix_pd = pandas.DataFrame(ttMatrix, index=vocab, columns=vocab)\n",
    "ttMatrix_pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d773caf",
   "metadata": {},
   "source": [
    "Based on term-term co-occurrence matrix, which pair(s) of words co-occur the most? \n",
    "\n",
    "**YOUR ANSWER**: \n",
    "\"north\" \"south\" are the the words that occurs the most"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a653fea",
   "metadata": {},
   "source": [
    "### Exercise 1.1.3 (1 point)\n",
    "Given some word $w_i$ occurring in the corpus, we can also consider the **context window** surrounding $w_i$. Supposing our fixed window size is $n$, then this is the $n$ preceding and $n$ subsequent words in that document, i.e. words $w_{i-n} \\dots w_{i-1}$ and $w_{i+1} \\dots w_{i+n}$. We build a *co-occurrence matrix* $M$, which is a symmetric word-by-word matrix in which $M_{ij}$ is the number of times $w_j$ appears inside $w_i$'s window. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a67a0a22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antarctic</th>\n",
       "      <th>arctic</th>\n",
       "      <th>bizarre</th>\n",
       "      <th>change</th>\n",
       "      <th>climate</th>\n",
       "      <th>heat</th>\n",
       "      <th>heating</th>\n",
       "      <th>heatwaves</th>\n",
       "      <th>high</th>\n",
       "      <th>north</th>\n",
       "      <th>pole</th>\n",
       "      <th>poles</th>\n",
       "      <th>see</th>\n",
       "      <th>simultaneous</th>\n",
       "      <th>south</th>\n",
       "      <th>strike</th>\n",
       "      <th>unusual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>antarctic</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arctic</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bizarre</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>change</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>climate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heat</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heating</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heatwaves</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pole</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poles</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simultaneous</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>south</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strike</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unusual</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              antarctic  arctic  bizarre  change  climate  heat  heating  \\\n",
       "antarctic           0.0     1.0      0.0     0.0      0.0   0.0      0.0   \n",
       "arctic              1.0     0.0      0.0     0.0      0.0   0.0      0.0   \n",
       "bizarre             0.0     0.0      0.0     0.0      0.0   0.0      0.0   \n",
       "change              0.0     0.0      0.0     0.0      1.0   0.0      0.0   \n",
       "climate             0.0     0.0      0.0     1.0      0.0   0.0      0.0   \n",
       "heat                0.0     0.0      0.0     0.0      0.0   0.0      0.0   \n",
       "heating             0.0     0.0      0.0     0.0      0.0   0.0      0.0   \n",
       "heatwaves           0.0     1.0      1.0     0.0      0.0   0.0      0.0   \n",
       "high                0.0     0.0      0.0     0.0      0.0   1.0      0.0   \n",
       "north               0.0     0.0      0.0     0.0      0.0   0.0      1.0   \n",
       "pole                0.0     0.0      0.0     0.0      0.0   0.0      2.0   \n",
       "poles               1.0     1.0      0.0     1.0      1.0   0.0      0.0   \n",
       "see                 0.0     0.0      0.0     0.0      0.0   0.0      0.0   \n",
       "simultaneous        0.0     0.0      0.0     0.0      0.0   0.0      0.0   \n",
       "south               0.0     0.0      0.0     0.0      0.0   0.0      1.0   \n",
       "strike              1.0     1.0      1.0     0.0      0.0   0.0      0.0   \n",
       "unusual             0.0     0.0      0.0     0.0      0.0   1.0      0.0   \n",
       "\n",
       "              heatwaves  high  north  pole  poles  see  simultaneous  south  \\\n",
       "antarctic           0.0   0.0    0.0   0.0    1.0  0.0           0.0    0.0   \n",
       "arctic              1.0   0.0    0.0   0.0    1.0  0.0           0.0    0.0   \n",
       "bizarre             1.0   0.0    0.0   0.0    0.0  0.0           0.0    0.0   \n",
       "change              0.0   0.0    0.0   0.0    1.0  0.0           0.0    0.0   \n",
       "climate             0.0   0.0    0.0   0.0    1.0  0.0           0.0    0.0   \n",
       "heat                0.0   1.0    0.0   0.0    0.0  0.0           0.0    0.0   \n",
       "heating             0.0   0.0    1.0   2.0    0.0  0.0           0.0    1.0   \n",
       "heatwaves           0.0   0.0    1.0   0.0    0.0  0.0           1.0    1.0   \n",
       "high                0.0   0.0    0.0   0.0    0.0  1.0           0.0    0.0   \n",
       "north               1.0   0.0    0.0   1.0    2.0  0.0           1.0    2.0   \n",
       "pole                0.0   0.0    1.0   0.0    0.0  0.0           0.0    2.0   \n",
       "poles               0.0   0.0    2.0   0.0    0.0  1.0           0.0    2.0   \n",
       "see                 0.0   1.0    0.0   0.0    1.0  0.0           0.0    1.0   \n",
       "simultaneous        1.0   0.0    1.0   0.0    0.0  0.0           0.0    0.0   \n",
       "south               1.0   0.0    2.0   2.0    2.0  1.0           0.0    0.0   \n",
       "strike              1.0   0.0    0.0   0.0    0.0  0.0           0.0    0.0   \n",
       "unusual             0.0   1.0    0.0   0.0    1.0  1.0           0.0    0.0   \n",
       "\n",
       "              strike  unusual  \n",
       "antarctic        1.0      0.0  \n",
       "arctic           1.0      0.0  \n",
       "bizarre          1.0      0.0  \n",
       "change           0.0      0.0  \n",
       "climate          0.0      0.0  \n",
       "heat             0.0      1.0  \n",
       "heating          0.0      0.0  \n",
       "heatwaves        1.0      0.0  \n",
       "high             0.0      1.0  \n",
       "north            0.0      0.0  \n",
       "pole             0.0      0.0  \n",
       "poles            0.0      1.0  \n",
       "see              0.0      1.0  \n",
       "simultaneous     0.0      0.0  \n",
       "south            0.0      0.0  \n",
       "strike           0.0      0.0  \n",
       "unusual          0.0      0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "def getTermTermWindowMatrix(corpus, window_size=2):\n",
    "    \"\"\" Note: Apply text normalisation before counting. \n",
    "              Each word in a document should be at the center of a window. \n",
    "              Words near edges will have a smaller number of co-occurring words. \n",
    "              For example, for the document \"Both North and South poles see unusually high heat\" with window size of 4,\n",
    "              \"north\" will co-occur with one word before (i.e. \"both\") and four words after (i.e. \"south\", \"poles\", \"see\", and \"unusally\")\n",
    "    \n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "            window_size (int): size of context window\n",
    "        Return:\n",
    "            M (numpy matrix of shape (size of vocabulary, size of vocabulary)): \n",
    "                Co-occurence matrix of word counts. \n",
    "                The vocabulary contains all the unique words in the corpus\n",
    "                The ordering of the words in the rows/columns should be the same\n",
    "\n",
    "    \"\"\"\n",
    "    # your code starts here\n",
    "    vectorizer = CountVectorizer(stop_words=stopwords.words('english')).fit(corpus)\n",
    "    vocab = [*vectorizer.vocabulary_]\n",
    "    vocab = sorted(vocab, reverse=False)\n",
    "    #Initialize it with zeros\n",
    "    tmMatrix = numpy.zeros((len(vocab), len(vocab)))\n",
    "    #check each document \n",
    "    for doc in corpus:\n",
    "        tokens = RegexpTokenizer(r'\\w+').tokenize(doc.lower())\n",
    "        tokens = [word for word in tokens if not word in stopwords.words('english')]  #if not then put in the tokens\n",
    "        #Now check each document and each words according to critea within the window size \n",
    "        for i in range(len(tokens)): #check each words\n",
    "            for j in range(window_size): #check according to the window size\n",
    "                if i - j - 1 >= 0 and tokens[i] not in stopwords.words('english') and tokens[i - j - 1] not in stopwords.words('english'):\n",
    "                    tmMatrix[vocab.index(tokens[i])][vocab.index(tokens[i - j - 1])] += 1 #check the center word and before if it is in limit then increment\n",
    "                if i + j + 1 < len(tokens) and tokens[i] not in stopwords.words('english') and tokens[i + j + 1] not in stopwords.words('english'):\n",
    "                    tmMatrix[vocab.index(tokens[i])][vocab.index(tokens[i + j + 1])] += 1 #Now check the center word and after if it is in limit then increment\n",
    "\n",
    "    return tmMatrix, vocab\n",
    "    # your code ends here\n",
    "ttwMatrix,vocab=getTermTermWindowMatrix(sents, 2)\n",
    "seettwMatrix = pandas.DataFrame(ttwMatrix, index=vocab, columns=vocab)\n",
    "seettwMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba6b5a7",
   "metadata": {},
   "source": [
    "## Exercise 1.2 Cosine similarity\n",
    "The benefit of vector semantics is that the similarity of two words can be computed as the cosine similarity between their vectors. Let's now compare how similar two words are. \n",
    "\n",
    "### Exercise 1.2.1 Write your own cosine similarity function (0.5 point)\n",
    "\n",
    "The cosine similarity metric between two vectors $v$ and $w$  can be computed as:\n",
    "$cosine(v,w )={\\mathbf {v} \\cdot \\mathbf {w}  \\over \\|\\mathbf {v} \\|\\|\\mathbf {w} \\|}={\\frac {\\sum \\limits _{i=1}^{n}{v_{i}w_{i}}}{{\\sqrt {\\sum \\limits _{i=1}^{n}{v_{i}^{2}}}}{\\sqrt {\\sum \\limits _{i=1}^{n}{w_{i}^{2}}}}}}$.\n",
    "\n",
    "Implement your cosine similarity function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18428c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.7999999999999998\n",
      "0.5773502691896258\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the cosine similarity function\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    #Now lets check according to the forula given above\n",
    "    return ((numpy.array(vec1)*numpy.array(vec2)).sum())/(math.sqrt((numpy.array(vec1)*numpy.array(vec1)).sum())*math.sqrt((numpy.array(vec2)*numpy.array(vec2)).sum()))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Your code continues here\n",
    "vec1 = tdMatrix[vocab.index('antarctic')]\n",
    "vec2 = tdMatrix[vocab.index('arctic')]\n",
    "print(cosine_similarity(vec1, vec2))\n",
    "\n",
    "vec1 = ttMatrix[vocab.index('antarctic')]\n",
    "vec2 = ttMatrix[vocab.index('arctic')]\n",
    "print(cosine_similarity(vec1, vec2))\n",
    "\n",
    "vec1 = ttwMatrix[vocab.index('antarctic')]\n",
    "vec2=ttwMatrix[vocab.index('arctic')]\n",
    "print(cosine_similarity(vec1, vec2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb97bc",
   "metadata": {},
   "source": [
    "What is the cosine similarity between \"antarctic\" and \"arctic\" if 1) using term-document occurrence matrix 2) using term-term co-occurrence matrix?\n",
    "\n",
    "**Your Answer**\n",
    "we got 1.0 for the term document, 0.799 for term-term co-occurrence matrix and 0.577 acording to the window size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c2ff1b",
   "metadata": {},
   "source": [
    "### Exercise 1.2.2 (0.5 point)\n",
    "\n",
    "Now we can calculate cosine similarity between words using a co-occurrence matrix. You can choose any previously constructed matrix for the similarity calculation. Rank all the words based on their similarity to the word *north*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec803dab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity_cal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>south</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pole</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simultaneous</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poles</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unusual</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heating</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heat</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heatwaves</th>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arctic</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>climate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>change</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bizarre</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strike</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>antarctic</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              similarity_cal\n",
       "south               1.000000\n",
       "high                0.577350\n",
       "pole                0.577350\n",
       "simultaneous        0.577350\n",
       "see                 0.577350\n",
       "poles               0.577350\n",
       "unusual             0.577350\n",
       "heating             0.577350\n",
       "heat                0.577350\n",
       "heatwaves           0.408248\n",
       "arctic              0.000000\n",
       "north               0.000000\n",
       "climate             0.000000\n",
       "change              0.000000\n",
       "bizarre             0.000000\n",
       "strike              0.000000\n",
       "antarctic           0.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank all the words by their similarity to word \"north\"\n",
    "\n",
    "# your code starts here\n",
    "#First let's make a an empty set\n",
    "similarity_cal = []\n",
    "for word in vocab:\n",
    "    if vocab.index(\"north\") == vocab.index(word):\n",
    "        similarity_cal.append(0.0) #cosine similarity between north and itself is always 1, and this code represents it as 0.0\n",
    "    else:\n",
    "            similarity_cal.append(cosine_similarity(tdMatrix[vocab.index('north')],tdMatrix[vocab.index(word)])) #calculates the similarity\n",
    "ranking = pandas.DataFrame(similarity_cal, index=vocab, columns=[\"similarity_cal\"])\n",
    "ranking.sort_values('similarity_cal', ascending=False)\n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06cc631",
   "metadata": {},
   "source": [
    "\n",
    "Does  the  calculated  cosine  similarity  reflect  semantic  similarity  or  relatedness?\n",
    "- If yes, to what extent?\n",
    "- If not, think of ways to get more reliable similarities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649c1612",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**: \n",
    "\n",
    "\n",
    "\n",
    "Cosine similarity measures the cosine of the angle based on their co-occurrence patterns in a corpus. While it can provide some indication of relatedness, it doesn't capture the full depth of semantic similarity, especially in a small corpus. It also depends on the context window size, the smaller context window size might miss broader semantic relationships.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221fd24",
   "metadata": {},
   "source": [
    "# Exercise 1.3 TF-IDF\n",
    "\n",
    "## Excercise 1.3.1 (1 point)\n",
    "For the above corpus, construct a TF-IDF weighted term-document matrix, using the formula 6.12 and 6.13 in the J&M book. \n",
    "- $tf(t,d) = \\log _{10}(count(t,d) +1)$ (6.12)\n",
    "- $idf(t) = \\log_{10} \\frac{N}{df(t)}$ (6.13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40d1cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to calculate TF-IDF we need to calculate the frequency of words and \n",
    "#term frequency (tf) for the word in that document \n",
    "\n",
    "\n",
    "# Function to calculate TF-IDF matrix\n",
    "def get_tfidfMatrix(corpus, vocab):\n",
    "    M = len(vocab)\n",
    "    N = len(corpus)\n",
    "    tfidf_matrix = np.zeros((M, N), dtype=float)  # Initialize the matrix with zeros\n",
    "\n",
    "    for i, word in enumerate(vocab):\n",
    "        idft = math.log10(N / np.count_nonzero(tdMatrix[i, :]))  # Calculate IDF for the word\n",
    "\n",
    "        for j in range(N):\n",
    "            tf = math.log10(tdMatrix[i, j] + 1)  # Calculate TF for the word in the document\n",
    "\n",
    "            # Calculate TF-IDF and store it in the matrix\n",
    "            tfidf_matrix[i, j] = tf * idft\n",
    "\n",
    "    return tfidf_matrix\n",
    "\n",
    "tfidf_matrix = get_tfidfMatrix(sents, vocab)\n",
    "\n",
    "tfidf_matrix_pd = pandas.DataFrame(tfidf_matrix, index=vocab, columns=[1,2,3,4,5])\n",
    "# your code ends here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79440cf7",
   "metadata": {},
   "source": [
    "Check the ranked list for *north* and see whether it changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f32f06f0-4169-4bc7-9fa5-6e031fd2545e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity_cal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>south</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pole</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simultaneous</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poles</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unusual</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heating</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heat</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heatwaves</th>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arctic</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>climate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>change</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bizarre</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strike</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>antarctic</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              similarity_cal\n",
       "south               1.000000\n",
       "high                0.577350\n",
       "pole                0.577350\n",
       "simultaneous        0.577350\n",
       "see                 0.577350\n",
       "poles               0.577350\n",
       "unusual             0.577350\n",
       "heating             0.577350\n",
       "heat                0.577350\n",
       "heatwaves           0.408248\n",
       "arctic              0.000000\n",
       "north               0.000000\n",
       "climate             0.000000\n",
       "change              0.000000\n",
       "bizarre             0.000000\n",
       "strike              0.000000\n",
       "antarctic           0.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rank all the words by their similarity to word \"north\"\n",
    "\n",
    "# your code starts here\n",
    "#First let's make a an empty set\n",
    "similarity_cal = []\n",
    "for word in vocab:\n",
    "    if vocab.index(\"north\") == vocab.index(word):\n",
    "        similarity_cal.append(0.0) #cosine similarity between north and itself is always 1, and this code represents it as 0.0\n",
    "    else:\n",
    "            similarity_cal.append(cosine_similarity(tfidf_matrix[vocab.index('north')],tfidf_matrix[vocab.index(word)])) #calculates the similarity\n",
    "ranking = pandas.DataFrame(similarity_cal, index=vocab, columns=[\"similarity_cal\"])\n",
    "ranking.sort_values('similarity_cal', ascending=False)\n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc256cd4-ab4e-4cad-a94f-53f532d6d8bc",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60efccf",
   "metadata": {},
   "source": [
    "## Exercise 1.3.2 (1 point)\n",
    "\n",
    "Let's use a bigger dataset which contains 2225 BBC news articles to construct TF-IDF term-document matrix. You may use your own function or [`sklearn.feature_extraction.text.TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) from the scikit library to generate TF-IDF weighted term-document matrix. When your matrix is ready, you may use [`sklearn.metrics.pairwise.cosine_similarity`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) to calculate the pair-wise cosine similarity among all the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b725e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary is 29421\n",
      "The shape of the term-document matrix is (29421, 2225)\n"
     ]
    }
   ],
   "source": [
    "#first lets load the data using pandas \n",
    "sents=pandas.read_csv('bbc-text.csv')\n",
    "#Then make sents and vectorize it in to get vocabs and then sort it out\n",
    "sents= sents['text']\n",
    "vectorizer = CountVectorizer().fit(sents.tolist())\n",
    "vocab = [*vectorizer.vocabulary_]\n",
    "vocab = sorted(vocab, reverse=False)\n",
    "\n",
    "tdoc_matrix = TfidfVectorizer().fit_transform(sents).toarray()\n",
    "tdoc_matrix = numpy.matrix(tdoc_matrix).transpose()\n",
    "tdoc_matrix = pandas.DataFrame(tdoc_matrix, index=vocab, columns=list(range(0, len(sents))))\n",
    "tdoc_matrix = tdoc_matrix.to_numpy()\n",
    "\n",
    "print('The size of the vocabulary is', len(vocab))\n",
    "print('The shape of the term-document matrix is', tdoc_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31793fb7",
   "metadata": {},
   "source": [
    "We can compute which words are most similar to *north*. Does this list of words make more sense now and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "644c0120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity_cal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mallon</th>\n",
       "      <td>0.441048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <td>0.441048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wades</th>\n",
       "      <td>0.441048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>0.441048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ne</th>\n",
       "      <td>0.441048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robb</th>\n",
       "      <td>0.441048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exerts</th>\n",
       "      <td>0.441048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tomaney</th>\n",
       "      <td>0.441048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>0.441048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>herron</th>\n",
       "      <td>0.377049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         similarity_cal\n",
       "mallon         0.441048\n",
       "metric         0.441048\n",
       "wades          0.441048\n",
       "487            0.441048\n",
       "ne             0.441048\n",
       "robb           0.441048\n",
       "exerts         0.441048\n",
       "tomaney        0.441048\n",
       "939            0.441048\n",
       "herron         0.377049"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the top 10 words that are most similar to word \"north\"\n",
    "\n",
    "# your code starts here\n",
    "# Rank all the words by their similarity to word \"north\"\n",
    "\n",
    "# your code starts here\n",
    "#First let's make a an empty set\n",
    "similarity_cal = []\n",
    "for word in vocab:\n",
    "    if vocab.index(\"north\") == vocab.index(word):\n",
    "        similarity_cal.append(0.0) #cosine similarity between north and itself is always 1, and this code represents it as 0.0\n",
    "    else:\n",
    "            similarity_cal.append(cosine_similarity(tdoc_matrix[vocab.index('north')],tdoc_matrix[vocab.index(word)])) #calculates the similarity\n",
    "ranking = pandas.DataFrame(similarity_cal, index=vocab, columns=[\"similarity_cal\"])\n",
    "ranking = ranking.sort_values('similarity_cal', ascending=False)\n",
    "ranking.head(10)\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# your code ends here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b344484f-26ed-45ef-9a1b-870d8b136c10",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5206eaef",
   "metadata": {},
   "source": [
    "Find another 3 pairs of words whose cosine similarity makes sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f51110f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between 'tv' and 'television' is [[0.33857192]]\n",
      "The similarity between 'technology' and 'technologies' is [[0.41697637]]\n",
      "The similarity between 'nintendo' and 'ds' is [[0.91591187]]\n"
     ]
    }
   ],
   "source": [
    "# Look for 3 pairs of words whose cosine similarities reflect their semantic similarity or relatedness.\n",
    "\n",
    "\n",
    "home_index = vocab.index(\"tv\")\n",
    "house_index = vocab.index(\"television\")\n",
    "similarity1 = cosine_similarity(tdoc_matrix[home_index].reshape(1, -1), tdoc_matrix[house_index].reshape(1, -1))\n",
    "print(\"The similarity between 'tv' and 'television' is\", similarity1)\n",
    "\n",
    "technology_index = vocab.index(\"technology\")\n",
    "technologies_index = vocab.index(\"technologies\")\n",
    "similarity2 = cosine_similarity(tdoc_matrix[technology_index].reshape(1, -1), tdoc_matrix[technologies_index].reshape(1, -1))\n",
    "print(\"The similarity between 'technology' and 'technologies' is\", similarity2)\n",
    "\n",
    "nintendo_index = vocab.index(\"nintendo\")\n",
    "ds_index = vocab.index(\"ds\")\n",
    "similarity3 = cosine_similarity(tdoc_matrix[nintendo_index].reshape(1, -1), tdoc_matrix[ds_index].reshape(1, -1))\n",
    "print(\"The similarity between 'nintendo' and 'ds' is\", similarity3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf1757",
   "metadata": {},
   "source": [
    "# Part II. Word2Vec word vectors\n",
    "\n",
    "Here, we explore the embeddings produced by word2vec. Please read J&M 6.8 or the [original paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) if you are interested in the details of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fbd4ab",
   "metadata": {},
   "source": [
    "## Exercise 2.1 Pre-train word2vec model\n",
    "\n",
    "Run the following script to load the word2vec vectors into memory. **Note**: This might take several minutes. If you run out of memory, try closing other applicaions or restart your machine to free more memory. \n",
    "\n",
    "Please note, the following experiments run with Gensim 4.2.0. If you are still running an old version of Gensim, please upgrade your Gensim library or check [Migrating from Gensim 3.x to 4](https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4) to adapt your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ea97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 3 million Word2Vec Vectors, pre-trained on Google news, each with the dimension of 300\n",
    "# This model may take a few minutes to load.\n",
    "\n",
    "import gensim.downloader as api\n",
    "start_time = time.time()\n",
    "w2v_google = api.load(\"word2vec-google-news-300\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f880003",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loaded vocab size {}\".format(len(w2v_google.index_to_key)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedd53f8",
   "metadata": {},
   "source": [
    "Once the model is loaded, you can extract the vector for individual words directly using `w2v_google['']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d4585",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google['north']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e956e1",
   "metadata": {},
   "source": [
    "One of the property of semantic embedding is that similar words are embedded close to each other. Use  `w2v_google.most_similar()` to identify the most similar words to *north*. Does this list make more sense to you? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bcf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for w,c in w2v_google.most_similar('north'):\n",
    "    print(w,c)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e545c5",
   "metadata": {},
   "source": [
    "Check a few more words to see whether their most similar words make sense to you and explain why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the computed most similar words make sense \n",
    "w2v_google.most_similar('WORD1')\n",
    "w2v_google.most_similar('WORD2')\n",
    "w2v_google.most_similar('WORD3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395facc-090b-48cd-94b9-237bb2e246de",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f427cb",
   "metadata": {},
   "source": [
    "## Word analogies\n",
    "\n",
    "An analogy explains one thing in terms of another to highlight the ways in which they are alike. For example, *paris* is similar to *france* in the same way that *rome* is to *italy*. Word2Vec vectors sometimes shows the ability of solving analogy problem of the form **a is to b as a* is to what?**.\n",
    "\n",
    "In the cell below, we show you how to use word vectors to find x. The `most_similar` function finds words that are most similar to the words in the `positive` list and most dissimilar from the words in the `negative` list. The answer to the analogy will be the word ranked most similar (largest numerical value). In the case below, the top one word *italy* is the answer, so this analogy is solved successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d152e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to answer the analogy -- paris : france :: rome : x\n",
    "print(w2v_google.most_similar(positive=['rome', 'france'], negative=['paris']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c654a",
   "metadata": {},
   "source": [
    "### Exercise 2.1.1 (0.5 point)\n",
    "Look for one analogy that can be solved successfully and one analogy that could not be solved using this pre-trained Word2Vec model. Check out [this paper](https://www.semanticscholar.org/paper/Efficient-Estimation-of-Word-Representations-in-Mikolov-Chen/330da625c15427c6e42ccfa3b747fb29e5835bf0) for inspirations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a55841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# successful case\n",
    "\n",
    "\n",
    "# failed case\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186fe0ba",
   "metadata": {},
   "source": [
    "## Visualising word analogies\n",
    "\n",
    "The following cell shows you how to use [tSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) to visualise a set of words based on their embeddings. You can also apply other dimensionality reduction methods (e.g. [sklearn.decomposition.TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)) to reduce the vectors from 300-dimensional to 2 dimensional. \n",
    "\n",
    "Please note, reducing dimensionality from 300 to 2 is a very challenging task. You can try different parameters in the tSNE and see their effects on the final visualisation. In particular, the visualisation is very sensitive to the perplexity value that you give. Please try a few different perplexity valuse and keep the one that gives the most reasonable visusalisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4515b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model, wordlist,p): # Create TSNE model and plot it\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    \n",
    "    for word in wordlist:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tokens=numpy.array(tokens)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=p, n_components=2, init='pca', n_iter=2500, random_state=23) \n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(18, 18)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "   \n",
    "wordlist=['beijing','china', 'london', 'uk', 'berlin', 'germany', 'rome', 'italy','tokyo', 'japan', ]\n",
    "tsne_plot(w2v_google, wordlist,len(wordlist)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dcd4c5",
   "metadata": {},
   "source": [
    "### Exercise 2.1.2 (0.5 point)\n",
    "Find another group analogies (at least 3 pairs of words) and see how they are visualised.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c486225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare at least 3 pairs of words   \n",
    "\n",
    "# your answer goes here\n",
    "\n",
    "wordlist=[]\n",
    "p=len(wordlist)-1\n",
    "tsne_plot(w2v_google,wordlist,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baa0b36",
   "metadata": {},
   "source": [
    "### Exercise 2.1.3  Synonyms and antonyms (0.5 point)\n",
    "\n",
    "\n",
    "\n",
    "Find three words (w1, w2, w3) so that \n",
    "- w1 and w2 are synonyms, \n",
    "- w1 and w3 are antonyms, \n",
    "- cosine_distance(w1, w2) > cosine_distance(w1, w3) or cosine_distance(w1, w2) $\\approx$ cosine_distance(w1, w3). \n",
    "\n",
    "Please give a possible explanation for why this has happened. \n",
    "\n",
    "You can use [`w2v_google.distance()`](https://radimrehurek.com/gensim/models/keyedvectors.html) function to compute the cosine distance between two words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b68367",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=''\n",
    "w2=''\n",
    "w3=''\n",
    "\n",
    "print(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2, w2v_google.distance(w1, w2)))\n",
    "print(\"Antonyms {}, {} have cosine distance: {}\".format(w1, w3, w2v_google.distance(w1, w3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee08b7",
   "metadata": {},
   "source": [
    "**Your answer**: \n",
    "\n",
    "Word2Vec and other vector space models use co-occurrences as the evidence for computing the semantic similarity and relatedness. Some antonyms are often used together in the same context, therefore their vector representations or embeddings are numerically close to each other (small distance). However, many synonyms  are not used often in the same textual context therefore will not have similar vectors, i.e. the cosine distance is higher.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a78159",
   "metadata": {},
   "source": [
    "### Exercise 2.1.4 Polysemous Words (0.5 point)\n",
    "\n",
    "Some words are polysemous, i.e. they have multiple meanings. For example the word *bank* can be a financial institute or the rising ground bordering a lake or river. Find a polysemous word whose top most similar words contains related words from multiple meanings. You should use the the [`wv_google.most_similar()`](https://radimrehurek.com/gensim/models/keyedvectors.html) function to compute the closet neighbours of the word. You may increase the number of neighbours in order to identify multiple groups of meanings. Submit the ranked word list and explained how the words are grouped into different meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25cd2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google.most_similar('run',topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b39d280",
   "metadata": {},
   "outputs": [],
   "source": [
    "focusword='run'\n",
    "wordlist=[focusword]\n",
    "for w in w2v_google.most_similar(focusword,topn=100):\n",
    "    wordlist.append(w[0])\n",
    "print(wordlist)\n",
    "tsne_plot(w2v_google,wordlist,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31efed6b",
   "metadata": {},
   "source": [
    "Look into literature and describe potential methods to address this polysymy issue in word embeddings. Please cite the papers that you refer to. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfbab96",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18347b85",
   "metadata": {},
   "source": [
    "## Exercise 2.2  Self-trained Word2Vec model\n",
    "\n",
    "The word2vec model that we have been using so far is pre-trained on Google news. This is suitable for applications involving general topics. However, for special domains, such as scientific or medical domain, some domain-specific semantics could not be captured in the pre-trained model. Fortunately, word2vec is pretty efficient in training from scratch. We will use two different datasets to observer the effect on the input corpus. \n",
    "\n",
    "Importance parameters are highlighted in bold. Please choose a few different values and see their effects.  \n",
    "\n",
    "class gensim.models.word2vec.Word2Vec(sentences=None, corpus_file=None, **vector_size=100**, alpha=0.025, **window=5**, **min_count=5**, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, **sg=0**, hs=0, **negative=5**, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)\n",
    "    \n",
    "Please check the [gensim documentation](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec) for more assistance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5cfb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most similar words to 'young' in Google news\n",
    "w2v_google.most_similar('young')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874bc89c",
   "metadata": {},
   "source": [
    "### Exercise 2.2.1 (1 point)\n",
    "\n",
    "We first train a word2vec model on the corpus consisting the abstracts from 111K astrophysics/astronomy articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6497b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This might take up a few minutes to train.\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "sentences=LineSentence('astro_norm.txt')\n",
    "\n",
    "start_time = time.time()\n",
    "# Train a word2vec model using the astro dataset\n",
    "# your code starts here\n",
    "\n",
    "\n",
    "# your code ends here\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19166f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_astro.wv.most_similar('young')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fc8a04",
   "metadata": {},
   "source": [
    "If all goes well, you may see *pms*, *proto* or *yso* among the top 10 most similar words to *young*. If you are curious, protostars and pre-main-sequence (PMS) stars are all [Young Stella Objects](https://en.wikipedia.org/wiki/Young_stellar_object)  (YSOs). Here, “young” means pre-main-sequence. For low-mass stars, this means ages of $10^5$ to $10^8$ years. [Ref](https://nexsci.caltech.edu/workshop/2003/2003_MSS/10_Thursday/mss2003_jensen.pdf)\n",
    "\n",
    "We then train a word2vec model on the corpus consisting of nearly 479K [Medline](https://www.nlm.nih.gov/medline/medline_overview.html) articles. Note, this corpus is rather big. If this is too much for your local machine, use UT's [JupyterLab](https://www.utwente.nl/en/service-portal/research-support/it-facilities-for-research/jupyterlab) or [Google Colab](https://colab.research.google.com/notebooks/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303b68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might take up half an hour to train!\n",
    "\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "sentences=LineSentence('medline_norm.txt')\n",
    "\n",
    "start_time = time.time()\n",
    "# Train a word2vec model using the astro dataset\n",
    "# your code starts here\n",
    "\n",
    "\n",
    "# your code ends here\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6bb758",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_medline.wv.most_similar('young')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40661b",
   "metadata": {},
   "source": [
    "Find another word and compute its most similar words based on different models. If you get different lists of words that are most similar to the target word, please explain why this happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb95a96",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee509ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google.most_similar('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892977b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_astro.wv.most_similar('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e78314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_medline.wv.most_similar('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ba7781",
   "metadata": {},
   "source": [
    "**YOUR ANSWER:** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6c763b",
   "metadata": {},
   "source": [
    "### Exercise 2.2.2 (1 point)\n",
    "\n",
    "Experiment with different parameters, for example, the vector size, the window size, the minimal count, skip-gram or CBOW, etc. Observe their effects on the quality of the word embeddings and/or computational cost. \n",
    "\n",
    "You can apply intrinsic evaluations to compare the quality of your models. For example, your can check the correlation with human opinion on word similarity or on word analogies. Check [gensim documentations](https://radimrehurek.com/gensim/models/keyedvectors.html) for more options.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352924ca",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef2b97",
   "metadata": {},
   "source": [
    "### Exercise 2.2.3 (0.5 point)\n",
    "Name at least 2 advantages of dense word vectors such as those calculated with Word2Vec over the vectors based on the term-document or term-term matrix as in Part I."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c4310",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f9337",
   "metadata": {},
   "source": [
    "### Exercise 2.2.4 (0.5 point)\n",
    "Name  at  least  2  limitations  of  the  Word2Vec  embedding  model.   Note,these limitations are not necessarily exclusive for Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e834151",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
