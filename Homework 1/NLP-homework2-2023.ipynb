{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cbIaeP9pX07"
   },
   "source": [
    "# Natural Language Processing - Assignment 2\n",
    "# Sentiment analysis for movie reviews\n",
    "\n",
    "This notebook was created for you to answer question 2, 3 and 4 from assignment 2. Please read the steps and the provided code carefully and make sure you understand them. \n",
    "\n",
    "The (red) comments at the beginning of each function explain what they should do, which parameters you should give as input and which variables should be returned by the function. After the (green) comments \"### student code here###' you should write your own code.\n",
    "\n",
    "**Please modify the next cell specifying your group number**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hIJFbDA1Qbi"
   },
   "source": [
    " *This is the Notebook of* ***Group 10*** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQfxb4pUNs1-"
   },
   "source": [
    "### Prerequisite - Libraries\n",
    "Make sure you have the needed libraries installed on your computer: scikit-learn, Pandas, NLTK..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KiI6RyOpX08"
   },
   "source": [
    "### Prerequisite - Load Data\n",
    "\n",
    "In the first step, we are going to load the data in a Pandas DataFrame. Pandas DataFrames are a useful way of storing data. DataFrames¬†are tables in which data can be accessed as columns, as rows or as individual cells. You can find more info on DataFrames here: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
    "\n",
    "Read the code below and make sure you understand what is happening. Run the code to load your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hX1AE_fJpX09"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "### student code here: import the needed modules from sci-kit learn ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eazU-uYcpX1B"
   },
   "outputs": [],
   "source": [
    "def get_path(filename):\n",
    "    \"\"\"\n",
    "    Makes a list of all the paths that fit the search requirement\n",
    "    \n",
    "    :param filename: A regular expression that defines the search requirement for the filenames\n",
    "    :return  Returns a list of all the pathnames\n",
    "    \"\"\"\n",
    "    # place the movies folder in the same directory as this notebook\n",
    "    current_directory = os.getcwd()\n",
    "    # if you are using Google Colab, you will have to change the above line\n",
    "    # to load the dataset¬†from your Google Drive\n",
    "\n",
    "    # glob.glob() is a pattern-matching path finder, it searches for the reviews in the movies folder based on a Regular Expression\n",
    "    paths = glob.glob(current_directory + '/movies/' + filename)\n",
    "    \n",
    "    if len(paths) == 0:\n",
    "        print('Your file list is empty. The code looks for the folder '+current_directory+'/movies, but could not find it.')\n",
    "    else: \n",
    "        print(\"Found \", len(paths), \"files\")\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RrcOjEdSpX1E"
   },
   "outputs": [],
   "source": [
    "def load_data(pathset):\n",
    "    \"\"\"\n",
    "    Loads the data into a dataframe\n",
    "    \n",
    "    :param pathset:  A list of paths\n",
    "    :return  A dataframe with three columns: Path, Review (Text) and Label\n",
    "    \"\"\"\n",
    "    # Files are named by sentiment (P for positive, N for negative)\n",
    "    pattern = re.compile('P-(train|test)[0-9]*.txt')\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    df = pd.DataFrame(columns = ['Path', 'Review', 'Label'])\n",
    "    for path in pathset:\n",
    "        if re.search(pattern, path):\n",
    "            text = open(path, \"r\").read()\n",
    "            reviews.append(text)\n",
    "            labels.append('Pos')\n",
    "        else:\n",
    "            text = open(path, \"r\").read()\n",
    "            reviews.append(text)\n",
    "            labels.append('Neg')\n",
    "    df['Path'] = pathset\n",
    "    df['Review'] = reviews\n",
    "    df['Label'] = labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cvGgLWN_pX1G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  1200 files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\lenovo/movies/train\\N-train001.txt</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\lenovo/movies/train\\N-train002.txt</td>\n",
       "      <td>This is a pale imitation of 'Officer and a Gen...</td>\n",
       "      <td>Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\lenovo/movies/train\\N-train003.txt</td>\n",
       "      <td>It seems ever since 1982, about every two or t...</td>\n",
       "      <td>Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\lenovo/movies/train\\N-train004.txt</td>\n",
       "      <td>Wow, another Kevin Costner hero movie. Postman...</td>\n",
       "      <td>Neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\lenovo/movies/train\\N-train005.txt</td>\n",
       "      <td>Alas, another Costner movie that was an hour t...</td>\n",
       "      <td>Neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Path  \\\n",
       "0  C:\\Users\\lenovo/movies/train\\N-train001.txt   \n",
       "1  C:\\Users\\lenovo/movies/train\\N-train002.txt   \n",
       "2  C:\\Users\\lenovo/movies/train\\N-train003.txt   \n",
       "3  C:\\Users\\lenovo/movies/train\\N-train004.txt   \n",
       "4  C:\\Users\\lenovo/movies/train\\N-train005.txt   \n",
       "\n",
       "                                              Review Label  \n",
       "0  Once again Mr. Costner has dragged out a movie...   Neg  \n",
       "1  This is a pale imitation of 'Officer and a Gen...   Neg  \n",
       "2  It seems ever since 1982, about every two or t...   Neg  \n",
       "3  Wow, another Kevin Costner hero movie. Postman...   Neg  \n",
       "4  Alas, another Costner movie that was an hour t...   Neg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the files in the Dataframe. This will take a while...\n",
    "paths = get_path('train/[NP]-train[0-9]*.txt')\n",
    "data = load_data(paths)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRRamA_8pX1K"
   },
   "source": [
    "### Part 2 - Tokenization\n",
    "\n",
    "In this step, you should write a tokenizer and compare it with an off-the-shelf one.\n",
    "\n",
    "#### Question 2.1 Making your own tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UkZwy1ATNs2F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'you', 'have', 'the', 'chance', ',', 'watch', 'it', '.', 'Although', ',', 'a', 'warning', ',', 'you', \"'\", 'll', 'cry', 'your', 'eyes', 'out', '.']\n",
      "['I', 'wish', 'life', 'would', 'be', 'a', 'bit', 'easy']\n",
      "['I', 'wish', 'to', 'go', 'to', 'Japan', 'every', 'once', 'in', 'a', 'year', '.', 'Wishes', 'do', 'come', 'true', '.', 'Right', '?']\n",
      "['Hello', ',', 'world', '!', 'How', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "def my_tokenizer(text):\n",
    "    \"\"\"\n",
    "    The implementation of your own tokenizer\n",
    "    \n",
    "    :param text:  A string with a sentence (or paragraph, or document...)\n",
    "    :return  A list of tokens\n",
    "    \"\"\"    \n",
    "    \n",
    "    \n",
    "    tokenized_text = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "    #Here I am using the regular expression for all the occurences, where it will split the text.\n",
    "    #  \\w+ are based on characters , | indicates OR expression and  [^\\w\\s] are for non-words such as punctuation.\n",
    "    \n",
    "    return tokenized_text\n",
    "\n",
    "sample_string0 = \"If you have the chance, watch it. Although, a warning, you'll cry your eyes out.\"\n",
    "sample_string1 = \"I wish life would be a bit easy\" \n",
    "sample_string2 = \"I wish to go to Japan every once in a year. Wishes do come true. Right?\" \n",
    "sample_string3 = \"Hello, world! How are you?\"\n",
    "print(my_tokenizer(sample_string0))\n",
    "print(my_tokenizer(sample_string1))\n",
    "print(my_tokenizer(sample_string2))\n",
    "print(my_tokenizer(sample_string3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pxI0gdoNs2G"
   },
   "source": [
    "#### Question 2.2 Using an off-the-shelf tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TXUTKVyqNs2H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_tokenizer = ['I', 'like', 'this', 'assignment', 'because', ':', '-', 'it', 'is', 'fun', ';', '-', 'it', 'helps', 'me', 'practice', 'my', 'Python', 'skills', '.']\n",
      "nltk_tokenizer = ['I', 'like', 'this', 'assignment', 'because', ':', '-', 'it', 'is', 'fun', ';', '-', 'it', 'helps', 'me', 'practice', 'my', 'Python', 'skills', '.']\n",
      "\n",
      "\n",
      "my_tokenizer = ['I', 'won', 'a', 'prize', ',', 'but', 'I', 'won', \"'\", 't', 'be', 'able', 'to', 'attend', 'the', 'ceremony', '.']\n",
      "nltk_tokenizer = ['I', 'won', 'a', 'prize', ',', 'but', 'I', 'wo', \"n't\", 'be', 'able', 'to', 'attend', 'the', 'ceremony', '.']\n",
      "\n",
      "\n",
      "my_tokenizer = ['‚Äú', 'The', 'strange', 'case', 'of', 'Dr', '.', 'Jekyll', 'and', 'Mr', '.', 'Hyde', '‚Äù', 'is', 'a', 'famous', 'book', '.', '.', '.', 'but', 'I', 'haven', \"'\", 't', 'read', 'it', '.']\n",
      "nltk_tokenizer = ['‚Äú', 'The', 'strange', 'case', 'of', 'Dr.', 'Jekyll', 'and', 'Mr.', 'Hyde', '‚Äù', 'is', 'a', 'famous', 'book', '...', 'but', 'I', 'have', \"n't\", 'read', 'it', '.']\n",
      "\n",
      "\n",
      "my_tokenizer = ['I', 'work', 'for', 'the', 'C', '.', 'I', '.', 'A', '.', '.', 'And', 'you', '?']\n",
      "nltk_tokenizer = ['I', 'work', 'for', 'the', 'C.I.A', '..', 'And', 'you', '?']\n",
      "\n",
      "\n",
      "my_tokenizer = ['OMG', '#', 'Twitter', 'is', 'sooooo', 'coooool', '<', '3', ':', '-', ')', '<', '-', '-', 'lol', '.', '.', '.', 'why', 'do', 'i', 'write', 'like', 'this', 'idk', 'right', '?', ':', ')', 'ü§∑', 'üòÇ', 'ü§ñ']\n",
      "nltk_tokenizer = ['OMG', '#', 'Twitter', 'is', 'sooooo', 'coooool', '<', '3', ':', '-', ')', '<', '--', 'lol', '...', 'why', 'do', 'i', 'write', 'like', 'this', 'idk', 'right', '?', ':', ')', 'ü§∑üòÇ', 'ü§ñ']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Now we are gonna compare the tokenizer you just wrote with the one from NLTK\n",
    "#if you installed NLTK but never downloaded the 'punkt' tokenizer, uncomment the following lines:\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def nltk_tokenizer(text):\n",
    "    \"\"\"\n",
    "    This function should apply the word_tokenize (punkt) tokenizer of nltk to the input text\n",
    "    \n",
    "    :param text:  A string with a sentence (or paragraph, or document...)\n",
    "    :return  A list of tokens\n",
    "    \"\"\"     \n",
    "  \n",
    "    \n",
    "    tokenized_text= word_tokenize(text)\n",
    "    \n",
    "    return tokenized_text\n",
    "\n",
    "test_sentences = [\"I like this assignment because:\\n-\\tit is fun;\\n-\\tit helps me practice my Python skills.\",\n",
    "        \"I won a prize, but I won't be able to attend the ceremony.\",\n",
    "        \"‚ÄúThe strange case of Dr. Jekyll and Mr. Hyde‚Äù is a famous book... but I haven't read it.\",\n",
    "        \"I work for the C.I.A.. And you?\",\n",
    "        \"OMG #Twitter is sooooo coooool <3 :-) <-- lol...why do i write like this idk right? :) ü§∑üòÇ ü§ñ\"]\n",
    "\n",
    "for test_string in test_sentences:\n",
    "    print(\"my_tokenizer =\", my_tokenizer(test_string))\n",
    "    print(\"nltk_tokenizer =\", nltk_tokenizer(test_string))\n",
    "    \n",
    "    #print(my_tokenizer(test_string))\n",
    "    \n",
    "    #print(nltk_tokenizer(test_string))\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUrQ_8EbNs2N"
   },
   "source": [
    "### Part 3 - Text classification with a unigram language model\n",
    "\n",
    "#### Training phase\n",
    "You now need to create the model and train it on the documents in the dataframe. Look at the scikit learn documentation to learn how to use the CountVectorizer and MultimodalNaiveBayes modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tQMy8K-MNs2N"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Student answer here ###\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', lowercase=True)\n",
    "X_train = data['Review']  # Make sure X_train contains the text data\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_counts, data['Label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrmIrKsbNs2O"
   },
   "source": [
    "#### Testing phase\n",
    "Now that you have a trained model, you need to test its performance.\n",
    "\n",
    "1. Load your test data.\n",
    "2. Classify your test data using the classifier you trained before.\n",
    "3. Compute the accuracy of your classifier on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NNZtd9aqNs2O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  100 files\n",
      "Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "# First, read all the test data from the files.  \n",
    "# Then classify it using the classifier you trained before\n",
    "# Finally, calculate the performance\n",
    "### Student code here ###\n",
    "\n",
    "test_paths = get_path('test/[NP]-test[0-9]*.txt')\n",
    "test_data = load_data(test_paths)\n",
    "X_test_counts = vectorizer.transform(test_data['Review'])\n",
    "\n",
    "predictions = classifier.predict(X_test_counts)\n",
    "accuracy = accuracy_score(test_data['Label'], predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2XPTWi1ytLW"
   },
   "source": [
    "Now train two more models: one without Laplace smoothing, and one where stopwords are removed. Then test them on the same test data, and compare the performance with the results you previously obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Laplace Smoothing (k = 1): 0.85\n",
      "Accuracy without Laplace Smoothing (k = 0): 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classifier_with_smoothing = MultinomialNB(alpha=1) #k=1\n",
    "classifier_with_smoothing.fit(X_train_counts, data['Label'])\n",
    "\n",
    "classifier_no_smoothing = MultinomialNB(alpha=0) #k=0\n",
    "classifier_no_smoothing.fit(X_train_counts, data['Label'])\n",
    "\n",
    "\n",
    "# Test the model with Laplace smoothing\n",
    "predictions_with_smoothing = classifier_with_smoothing.predict(X_test_counts)\n",
    "accuracy_with_smoothing = accuracy_score(test_data['Label'], predictions_with_smoothing)\n",
    "print(\"Accuracy with Laplace Smoothing (k = 1):\", accuracy_with_smoothing)\n",
    "\n",
    "# Test the model without Laplace smoothing\n",
    "predictions_no_smoothing = classifier_no_smoothing.predict(X_test_counts)\n",
    "accuracy_no_smoothing = accuracy_score(test_data['Label'], predictions_no_smoothing)\n",
    "print(\"Accuracy without Laplace Smoothing (k = 0):\", accuracy_no_smoothing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Stop Words Removed: 0.85\n",
      "Accuracy without Stop Words Removed: 0.85\n"
     ]
    }
   ],
   "source": [
    "#Now we will check the perfomance with removing stop words vs without removing them\n",
    "#Lets first train it\n",
    "vectorizer_no_stopwords= CountVectorizer(stop_words='english',lowercase=True)\n",
    "X_train_no_stopwords= data['Review']\n",
    "X_train_counts_no_stopwords = vectorizer_no_stopwords.fit_transform(X_train_no_stopwords)\n",
    "\n",
    "classifer_no_stopwords= MultinomialNB()\n",
    "classifer_no_stopwords.fit(X_train_counts_no_stopwords, data['Label'])\n",
    "\n",
    "#Now lets check the accuracy\n",
    "# Test the model with stop words removed\n",
    "X_test_counts_no_stopwords = vectorizer_no_stopwords.transform(test_data['Review'])\n",
    "predictions_no_stopwords = classifer_no_stopwords.predict(X_test_counts_no_stopwords)\n",
    "accuracy_no_stopwords = accuracy_score(test_data['Label'], predictions_no_stopwords)\n",
    "print(\"Accuracy with Stop Words Removed:\", accuracy_no_stopwords)\n",
    "\n",
    "# Test the model without stop words removed\n",
    "predictions = classifier.predict(X_test_counts)\n",
    "accuracy = accuracy_score(test_data['Label'], predictions)\n",
    "print(\"Accuracy without Stop Words Removed:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Preprocessing: 0.85\n",
      "Accuracy without Preprocessing: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Create a CountVectorizer without lowercase normalization and stop word removal\n",
    "vectorizer_no_preprocessing = CountVectorizer(lowercase=False, stop_words=None)\n",
    "X_train_no_preprocessing = data['Review']\n",
    "X_train_counts_no_preprocessing = vectorizer_no_preprocessing.fit_transform(X_train_no_preprocessing)\n",
    "\n",
    "# Train a classifier with the unprocessed text\n",
    "classifier_no_preprocessing = MultinomialNB()\n",
    "classifier_no_preprocessing.fit(X_train_counts_no_preprocessing, data['Label'])\n",
    "\n",
    "#Now lets test the model\n",
    "X_test_counts = vectorizer.transform(test_data['Review'])\n",
    "predictions = classifier.predict(X_test_counts)\n",
    "accuracy = accuracy_score(test_data['Label'], predictions)\n",
    "print(\"Accuracy with Preprocessing:\", accuracy)\n",
    "\n",
    "X_test_counts_no_preprocessing = vectorizer_no_preprocessing.transform(test_data['Review'])\n",
    "predictions_no_preprocessing = classifier_no_preprocessing.predict(X_test_counts_no_preprocessing)\n",
    "accuracy_no_preprocessing = accuracy_score(test_data['Label'], predictions_no_preprocessing)\n",
    "print(\"Accuracy without Preprocessing:\", accuracy_no_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDCgEfYgNs2Q"
   },
   "source": [
    "### Part 4 - Text classification with a bigram language model\n",
    "\n",
    "Now we will classify the same dataset again, but this time with a bigram language model. \n",
    "\n",
    "#### Training phase\n",
    "Build a Na√Øve Bayes classifier that uses bigrams instead of single words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZSIam3ObNs2Q"
   },
   "outputs": [],
   "source": [
    "### Student code here ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReyUDT1dNs2R"
   },
   "source": [
    "#### Testing phase\n",
    "As before, calculate the performance on your test data, and notice the difference with the previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "z6rkqDJENs2R"
   },
   "outputs": [],
   "source": [
    "### Student code here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gY3K9vaB0Vzk"
   },
   "source": [
    "### Trigrams\n",
    "When I asked students how to improve the classification performance on this dataset, the first question was always \"use trigrams\" (or even higher-order n-grams). Let's try how much of an improvement that would be, by training a trigram model and testing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "U7htbTfeNs2S"
   },
   "outputs": [],
   "source": [
    "### Student code here ###\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_2021_Homework2_FINAL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
