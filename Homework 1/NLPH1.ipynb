{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1efe937",
   "metadata": {},
   "source": [
    "Importing the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7af9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8a387",
   "metadata": {},
   "source": [
    "Initialize the Porter Stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eef301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b17b2d",
   "metadata": {},
   "source": [
    "Read the List of Words from File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ab4d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sorted_types.txt', 'r') as file:\n",
    "    words = file.read().split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dcc211",
   "metadata": {},
   "source": [
    "Stemming and Keeping Unique Stems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b68394f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_stems = set()\n",
    "for word in words:\n",
    "    stem = stemmer.stem(word)\n",
    "    unique_stems.add(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "599ce3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original word types: 245\n",
      "Number of unique stems after stemming: 223\n"
     ]
    }
   ],
   "source": [
    "no_unique_stems = len(unique_stems)\n",
    "\n",
    "print(\"Number of original word types:\", len(words))\n",
    "print(\"Number of unique stems after stemming:\", no_unique_stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d036aa37",
   "metadata": {},
   "source": [
    "Printing Unique Stems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74b475c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cue\n",
      "specifi\n",
      "when\n",
      "in\n",
      "substitut\n",
      "stem\n",
      "follow\n",
      "slash\n",
      "determin\n",
      "explor\n",
      "standard\n",
      "upper\n",
      "express\n",
      "document\n",
      "match\n",
      "applic\n",
      "if\n",
      "task\n",
      "introduc\n",
      "charact\n",
      "similar\n",
      "segment\n",
      "are\n",
      "mean\n",
      "same\n",
      "regular\n",
      "chang\n",
      "will\n",
      "test\n",
      "underlin\n",
      "delimit\n",
      "we'll\n",
      "variant\n",
      "search\n",
      "or\n",
      "thi\n",
      "unsung\n",
      "may\n",
      "contain\n",
      "text\n",
      "other\n",
      "process\n",
      "suffix\n",
      "exclam\n",
      "these\n",
      "is\n",
      "complex\n",
      "string\n",
      "tester\n",
      "extend\n",
      "need\n",
      "delet\n",
      "s\n",
      "becom\n",
      "sing\n",
      "design\n",
      "come\n",
      "collect\n",
      "lemma\n",
      "practic\n",
      "sung\n",
      "an\n",
      "final\n",
      "period\n",
      "littl\n",
      "form\n",
      "that\n",
      "the\n",
      "singl\n",
      "take\n",
      "with\n",
      "through\n",
      "mani\n",
      "first\n",
      "and\n",
      "algebra\n",
      "case\n",
      "map\n",
      "distanc\n",
      "strip\n",
      "speech\n",
      "lemmat\n",
      "corpu\n",
      "throughout\n",
      "woodchuck\n",
      "by\n",
      "gener\n",
      "includ\n",
      "exampl\n",
      "it\n",
      "a\n",
      "two\n",
      "variat\n",
      "of\n",
      "simpler\n",
      "slightli\n",
      "pattern\n",
      "refer\n",
      "emac\n",
      "compar\n",
      "i'm\n",
      "onlin\n",
      "unix\n",
      "parser\n",
      "japanes\n",
      "also\n",
      "use\n",
      "number\n",
      "particularli\n",
      "everi\n",
      "word\n",
      "so\n",
      "between\n",
      "up\n",
      "anoth\n",
      "not\n",
      "input\n",
      "subset\n",
      "normal\n",
      "point\n",
      "exact\n",
      "shorten\n",
      "edit\n",
      "be\n",
      "call\n",
      "languag\n",
      "buttercup\n",
      "metric\n",
      "algorithm\n",
      "recogn\n",
      "like\n",
      "way\n",
      "note\n",
      "sang\n",
      "fig\n",
      "comput\n",
      "correct\n",
      "sensit\n",
      "more\n",
      "spell\n",
      "line\n",
      "root\n",
      "break\n",
      "show\n",
      "describ\n",
      "lower\n",
      "surfac\n",
      "base\n",
      "handi\n",
      "on\n",
      "one\n",
      "individu\n",
      "grep\n",
      "simpl\n",
      "have\n",
      "character\n",
      "common\n",
      "see\n",
      "function\n",
      "recognit\n",
      "version\n",
      "verb\n",
      "from\n",
      "your\n",
      "sequenc\n",
      "corefer\n",
      "how\n",
      "set\n",
      "despit\n",
      "command-lin\n",
      "don't\n",
      "treat\n",
      "measur\n",
      "which\n",
      "but\n",
      "simplest\n",
      "differ\n",
      "just\n",
      "ani\n",
      "than\n",
      "would\n",
      "we\n",
      "their\n",
      "notat\n",
      "can\n",
      "mainli\n",
      "they\n",
      "success\n",
      "scienc\n",
      "basic\n",
      "tool\n",
      "formal\n",
      "insert\n",
      "ha\n",
      "all\n",
      "substr\n",
      "consist\n",
      "arab\n",
      "regex\n",
      "essenti\n",
      "to\n",
      "been\n",
      "processor\n",
      "into\n",
      "urgl\n",
      "out\n",
      "end\n",
      "difficult\n",
      "space\n",
      "there\n",
      "some\n",
      "return\n",
      "type\n",
      "morpholog\n",
      "kind\n",
      "for\n",
      "often\n",
      "resolut\n",
      "onli\n",
      "sentenc\n",
      "part\n",
      "distinct\n",
      "token\n"
     ]
    }
   ],
   "source": [
    "for stem in unique_stems:\n",
    "    print(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0c670",
   "metadata": {},
   "source": [
    "# lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c90fc2",
   "metadata": {},
   "source": [
    "Importing the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af7f8349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e711641",
   "metadata": {},
   "source": [
    "Initialize the WordNetLemmatizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c35d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154c6675",
   "metadata": {},
   "source": [
    "Lemmatization and Keeping Unique lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41dbfe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_lemmas = set()\n",
    "for word in words:\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    unique_lemmas.add(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a281c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original word types: 245\n",
      "Number of unique lemmas after lemmatization: 232\n"
     ]
    }
   ],
   "source": [
    "no_unique_lemmas = len(unique_lemmas)\n",
    "\n",
    "print(\"Number of original word types:\", len(words))\n",
    "print(\"Number of unique lemmas after lemmatization:\", no_unique_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9286cc52",
   "metadata": {},
   "source": [
    "Printing Unique Lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe77330d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cue\n",
      "following\n",
      "when\n",
      "in\n",
      "includes\n",
      "command-line\n",
      "handy\n",
      "this\n",
      "sequence\n",
      "particularly\n",
      "slash\n",
      "language\n",
      "surface\n",
      "sensitive\n",
      "single\n",
      "substring\n",
      "upper\n",
      "document\n",
      "match\n",
      "if\n",
      "task\n",
      "similar\n",
      "despite\n",
      "science\n",
      "designed\n",
      "emacs\n",
      "explore\n",
      "are\n",
      "mean\n",
      "same\n",
      "regular\n",
      "characterizing\n",
      "will\n",
      "test\n",
      "we'll\n",
      "variant\n",
      "search\n",
      "any\n",
      "or\n",
      "arabic\n",
      "unsung\n",
      "may\n",
      "shortened\n",
      "coreference\n",
      "text\n",
      "other\n",
      "suffix\n",
      "many\n",
      "these\n",
      "is\n",
      "complex\n",
      "notation\n",
      "string\n",
      "tester\n",
      "need\n",
      "s\n",
      "specifying\n",
      "sing\n",
      "come\n",
      "lemma\n",
      "sung\n",
      "searching\n",
      "that\n",
      "an\n",
      "determining\n",
      "lemmatization\n",
      "period\n",
      "form\n",
      "individual\n",
      "insertion\n",
      "the\n",
      "tokenization\n",
      "take\n",
      "sentence\n",
      "through\n",
      "with\n",
      "application\n",
      "first\n",
      "and\n",
      "case\n",
      "map\n",
      "only\n",
      "finally\n",
      "every\n",
      "speech\n",
      "called\n",
      "strip\n",
      "formally\n",
      "throughout\n",
      "generally\n",
      "woodchuck\n",
      "by\n",
      "it\n",
      "distance\n",
      "slightly\n",
      "a\n",
      "two\n",
      "of\n",
      "containing\n",
      "simpler\n",
      "pattern\n",
      "another\n",
      "different\n",
      "recognize\n",
      "used\n",
      "i'm\n",
      "underline\n",
      "compare\n",
      "unix\n",
      "parser\n",
      "also\n",
      "number\n",
      "word\n",
      "so\n",
      "corpus\n",
      "between\n",
      "up\n",
      "not\n",
      "input\n",
      "extended\n",
      "morphologically\n",
      "subset\n",
      "point\n",
      "becomes\n",
      "processing\n",
      "exact\n",
      "expression\n",
      "describing\n",
      "edit\n",
      "be\n",
      "change\n",
      "lemmatizer\n",
      "buttercup\n",
      "metric\n",
      "algorithm\n",
      "collection\n",
      "like\n",
      "variation\n",
      "differently\n",
      "way\n",
      "stemming\n",
      "note\n",
      "sang\n",
      "deletion\n",
      "fig\n",
      "more\n",
      "line\n",
      "root\n",
      "show\n",
      "lower\n",
      "example\n",
      "on\n",
      "refers\n",
      "one\n",
      "recognition\n",
      "grep\n",
      "japanese\n",
      "have\n",
      "character\n",
      "common\n",
      "see\n",
      "based\n",
      "function\n",
      "version\n",
      "verb\n",
      "from\n",
      "delimited\n",
      "mainly\n",
      "your\n",
      "set\n",
      "how\n",
      "breaking\n",
      "don't\n",
      "treat\n",
      "which\n",
      "useful\n",
      "but\n",
      "simplest\n",
      "measure\n",
      "essential\n",
      "just\n",
      "than\n",
      "difference\n",
      "would\n",
      "exclamation\n",
      "we\n",
      "their\n",
      "substitution\n",
      "can\n",
      "they\n",
      "edits\n",
      "success\n",
      "basic\n",
      "tool\n",
      "returning\n",
      "ha\n",
      "online\n",
      "all\n",
      "consist\n",
      "resolution\n",
      "standardization\n",
      "correction\n",
      "segmentation\n",
      "algebraic\n",
      "little\n",
      "sings\n",
      "practical\n",
      "regex\n",
      "to\n",
      "been\n",
      "introduce\n",
      "into\n",
      "processor\n",
      "out\n",
      "urgl\n",
      "end\n",
      "difficult\n",
      "space\n",
      "there\n",
      "some\n",
      "return\n",
      "type\n",
      "kind\n",
      "for\n",
      "computer\n",
      "often\n",
      "spelling\n",
      "part\n",
      "distinct\n",
      "normalization\n",
      "using\n",
      "simple\n"
     ]
    }
   ],
   "source": [
    "for lem in unique_lemmas:\n",
    "    print(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a77732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
